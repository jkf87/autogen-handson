{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkf87/autogen-handson/blob/main/%5Blangchain%5Dmap%2C_reduce_%EC%9C%A0%ED%8A%9C%EB%B8%8C_%EC%98%81%EC%83%81_%EC%9A%94%EC%95%BD_%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YouTube Video Summarization\n",
        "- 유튜브 비디오 내용 요약 인공지능 만들기\n",
        "\n",
        "\n",
        "1. 유튜브 영상 다운로드 (음성만 다운로드, mp3) - PyTube\n",
        "2. Speech to Text (Transcribe) - OpenAI Whisper (Local)\n",
        "3. **Map-reduce** summariation - LangChain, OpenAI ChatGPT API\n",
        "\n",
        "\n",
        "### 출처 유튜브 [빵형의 개발도상국](https://www.youtube.com/@bbanghyong)\n"
      ],
      "metadata": {
        "id": "78Xoo7NZTn6F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEWAygaxWfZs",
        "outputId": "6a4a7ed1-f8f0-4c14-9808-8ae1b7737bf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [\u001b[0m\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Connecting to security.ubuntu.com (185.125.190.39)] [Connecting to ppa.laun\u001b[0m\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,269 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,347 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 2,957 kB in 2s (1,297 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "18 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.3/794.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install -q openai-whisper pytube\n",
        "!pip install -q openai tiktoken langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download YouTube video\n",
        "\n",
        "[ChatGPT 파인튜닝 심리상담 챗봇 만들기 (비용, 시간, 데이터셋, 방법 소개)](https://youtu.be/sTC_srIwhIw?si=znP9FavxST5p0RFZ) - 빵형의 개발도상국 유튜브 채널"
      ],
      "metadata": {
        "id": "gdwIV5IDUM_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "yt = YouTube('https://youtu.be/sTC_srIwhIw?si=_BHcN3eH3lFhtdtZ')\n",
        "\n",
        "yt.streams.filter(only_audio=True).first().download(\n",
        "    output_path='.', filename='input.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Z0MVmwOQX0Ko",
        "outputId": "17e2841f-39af-451c-86d9-2dd54daa85bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/./input.mp3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribe"
      ],
      "metadata": {
        "id": "YkFBJYpGUQoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "id": "qVlpeceUXPkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30초 이상 오디오 데이터 처리 방법\n",
        "\n",
        "https://github.com/openai/whisper/discussions/136"
      ],
      "metadata": {
        "id": "_N-P3paimK7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\"input.mp3\")\n",
        "\n",
        "result[\"text\"][:300]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "NolAjGyKXhxT",
        "outputId": "d2d5261d-bd39-4827-cb85-af9b2007ae17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" This is the video where we are going to discuss GPT-4 vision prompt engineering techniques. At this point, there are not a lot of tutorials on GPT-4 vision prompt engineering techniques, but Microsoft recently put together very detailed paper on GPT-4 vision that's called the dawn of the LMM's. The\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Map-reduce summarization\n",
        "\n",
        "https://python.langchain.com/docs/use_cases/summarization#option-2-map-reduce\n",
        "\n",
        "한글 설명: https://teddylee777.github.io/langchain/langchain-tutorial-07/\n",
        "\n",
        "![](https://i.imgur.com/2jJ0unq.jpg)"
      ],
      "metadata": {
        "id": "9SemXn9jmRlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text splitter"
      ],
      "metadata": {
        "id": "f2qcpBZ9mY1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "# 이 객체는 텍스트를 나누어 주어진 청크 크기에 따라 여러 부분으로 분할합니다.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # 각 청크의 최대 길이를 설정합니다.\n",
        "    chunk_overlap=50,  # 청크 간에 겹치는 문자 수를 설정합니다.\n",
        "    length_function=len,  # 청크 길이를 측정하는 함수를 설정합니다.\n",
        ")\n",
        "\n",
        "# 텍스트를 분할하고, 각 부분을 Document 객체로 변환합니다.\n",
        "docs = [Document(page_content=x) for x in text_splitter.split_text(result[\"text\"])]\n",
        "\n",
        "# 분할된 문서들을 더욱 작은 부분들로 나눕니다.\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "# 분할된 문서의 개수를 출력합니다.\n",
        "len(split_docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XWJ3toqh1kZ",
        "outputId": "6c935a08-1bff-4306-8240-71b1a16359e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVjDfLm4jVxj",
        "outputId": "b6ed15a5-b3f1-4349-e18a-1fbe74a16c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"This is the video where we are going to discuss GPT-4 vision prompt engineering techniques. At this point, there are not a lot of tutorials on GPT-4 vision prompt engineering techniques, but Microsoft recently put together very detailed paper on GPT-4 vision that's called the dawn of the LMM's. The large multi-model models. From that paper, there is a particular segment that discusses about prompt engineering techniques, which we are going to go over in this video. There are going to be four different kinds of prompt engineering techniques. The first one is called following text instructions. The second one is called visual pointing or visual referral prompting and then the third one is called few short in prompt learning or in context learning and then the final one fourth one is the visual plus text, the combination of both and we are going to discuss all these four with the examples that Microsoft has given us in this paper. Let's get started. The first one you can see that it says\"),\n",
              " Document(page_content=\"started. The first one you can see that it says following text instructions. Imagine you have got an image like this. Once you upload the image, you can say describe the image with a sentence consisting of three or six or nine words. So when you do like this, it is going to give you three that's the answer for three six six for six and nine for nine. So once you upload the image and prompt this and GPT-4 vision is going to give you all these three outputs. So you can see how it follows the text, the instruction that has been given very diligently with the three words, the three words, the six words and then the nine words. This is quite helpful if you're especially writing like a more marketing copy or like a copy writing item. So this is super helpful. The next one is you can say write a sentence for the image consisting only the words starting with letter B, D and T. You can see it starts with B. The B sidebar was beautiful blue backdrop, D, dining deck, displays, delightful drinks,\"),\n",
              " Document(page_content=\"D, dining deck, displays, delightful drinks, distant docks, T, tabletop, terrace, teaming with tropical tranquility. So it's really really good. But on the other hand, if you want to go ahead and then see what would the missing image look like. And then you can see that you've got like a diamond here and like a star here and with a dot and it says the missing image would likely be a variation of the diamond or a starship as seen in other three images. It could be a diamond with a different pattern or color and the star with different. It is difficult to say for sure without more context, but it would likely follow the same theme or other images. So you did not get the answer. So you just simply uploaded the image and said what would the missing image look like and you did not get the answer. So now you can go ahead and then add the details to the prompt. Here are the three sub images arranged in a two by two matrix. The first look at the two images in the first column in the top left\"),\n",
              " Document(page_content=\"two images in the first column in the top left and the bottom left. Then find out the pattern in the first column based on the left two images. Next, use the found pattern and the image on the top right to infer the missing figure. Finally, describe what would the missing image look like. Once you add these details, you can see that it manages to give you the answer saying, therefore, the missing image in the bottom right would be a star with the dot in the center similar to the image on the top right, but with a dot in the center. So this is just with simply a prompt with instructions. How does it follow the instruction? And the next one is following instruction is one of the things that people love to do is get a structured response back from GPT-4 vision or even GPT-4 for that matter or any LLMs. So you give an input and you get an output in a structured format. And in this case, it's just JSON format. Please read the text in the image. Return the following information in JSON\"),\n",
              " Document(page_content=\"image. Return the following information in JSON format. And you can see that the format has been given as part of the prompt and this is the input. The most important thing is to give it a structure that will give you the exact structure as response. So when you give the structure, you can see that it gave you D. It gave you all the other information. And in fact, you go ahead one step further and then say put any instant when the information is not available and then from the image. So that will help you get consistent output for all the images. For example, let's say you have got 10 images and you want to get all the 10 images into a structured output at this point. You do not have an API access. So still you can get it by having the same prompt with the same input JSON format for which GPT-4 is going to fill you the data and give you the data as an output. So you can just add further information in it and then you can get the information. The next one part of the following text\"),\n",
              " Document(page_content=\"The next one part of the following text instruction is you can just simply say count the number of apples in the image. So it's going to count 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11. But still it says 12 images and that's where we have got a very classical chain of thought and we are going to say let's take think step by step and it gives you the same output. So you said step by step it counted the same one. Now you can go ahead and then add more instruction to it. So instead of simply saying let's think step by step you can say let's count the apples row by row. So it's going to count row by row 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 8 plus 3 is 11. So you can see once you say that it is saying okay the first row has got four apples. The second row has got three apples that the third row has got four apples. Despite the answer being correct you can see that the way it approves is wrong. So you can change the prompt. Count the number of apples in the image. First count how many rows of apples\"),\n",
              " Document(page_content=\"in the image. First count how many rows of apples are there then count the apples in each row and finally sum them up together to get the total number. So first you're saying count simply the number of rows then count each apple in each row and then sum them up together and it does it but it does a mistake here you can see. Now you're going to add the traditional you are an expert thing. You are an expert in counting things in the image. Let's count the number of apples in the image below row by row to be sure we have the right answer. First you're going to say four and it's going to be four is going to be three. Total number of images in the apples, 11 apples finally we arrived at the solution. The reason why they have gone through step by step is for us to understand what is happening how you can steer the prompt instructions so that you can get the right output and with this we are going to go into the next section which is going to be the visual pointing. What is visual prompting\"),\n",
              " Document(page_content='be the visual pointing. What is visual prompting or visual pointing, visual referring prompting. Human beings have a traditional way of pointing out something or talking about something. For example if I were to tell you hey look up I would just like point up like this and if I were to tell you hey look at look at this then I would probably say look at this like this. So we have a habit of you know either marking something like showing something and then asking like talking to fellow humans. So what visual pointing and visual referring prompting talks about is exactly the same thing. So you can just point out something like for example you can have this arrow the box like mark something like circle something hand drawing and you can have all these things and ask you to give you a response back like how does it work you can upload an image and then say describe the pointed region in the image. So first GPT-4 vision understands what is a pointed region and then it starts explaining you'),\n",
              " Document(page_content=\"pointed region and then it starts explaining you what is there in the highlighted region. So it says okay the pointed region in the image is a row of hanging lights on a wire and the next thing that you can do is you can upload like a table here and then you can say describe the pointed region in the image. It's a very simple prompt if you see the prompt all you're saying is describe the pointed region in the image but just because you're pointing it it actually understands that is what you're interested in. The highlighted red region in the image is a table cell that contains numerical value. You can see it talks about all these details. The next thing is you can have multiple annotations in an image. What is in the circled glass object one or object two. Describe what is object one object two then check what is in the circle glass. You can see this is object one and you have said this is object two and you are seeing this is the circle image. So it has to figure out what is in the\"),\n",
              " Document(page_content=\"image. So it has to figure out what is in the object one what is in the object two and what is inside the glass whether it is object one or object two and it does the job of explaining what is inside it. The next one is answer the questions in the following figure provide immediate steps. Okay, I've got like a right angle triangle. I guess yeah how long this is the edge and what is the angle here and it goes on calculating the entire details. So what is happening here you're uploading an image and you are giving it like with an annotation or like pointing and then asking it to figure out what is it. It's very powerful especially if you think about like how you can use this as something and then use it for like help like visually challenge people. The next one is visual and text prompting which is going to be combination of both. For example you can say find out the pattern in the first column and use it to infer the missing figure in the second column. So it's going it has like a\"),\n",
              " Document(page_content=\"in the second column. So it's going it has like a combination of pointing but also the text prompting like the instructions that we gave it first. So you can see it goes on with the details. So this is something that we figured out from the first section that we learned and then you can see that it it just like with the image that it knows what to do and you also like give extra information and it figures out the final answer for you and the same way you can again go ahead and say okay find out the pattern in the first column use it to infer the missing figure in the second column and you can just give more text and give the details here and then give everything in there and then it figures out the answer. So one thing that not a lot of people have realized it's a with GPT4 vision you can upload more than one image and the image can also have text and instructions. So this is what has happened here like for example you have got the image but you also have got the text as part of the\"),\n",
              " Document(page_content=\"but you also have got the text as part of the image in and itself. So it helps the GPT4 vision to figure out what you want and the final section is the in context few short learning. It has two things. One is the in context learning and then the few short learning. It's quite detailed so I'll link the paper for you to go through it but the main idea here is that when you upload the image instead of giving a simple prompt like this which ideally you know it will not give you the right answer because you know sometimes it gets confused with a lot of things available here. What is the read of the speed meter? So it says approximately 22 miles per hour which is not true because you can see the speed meter does not go above 22. How do you figure out as a human being? As a human being what I do first first I look at the speedometer then I try to look at where the you know the tick is or the the yellow color thing is and then I try to point out like figure out where it is and it doesn't have\"),\n",
              " Document(page_content=\"like figure out where it is and it doesn't have a reading. So I try to figure out where the next reading is and then I try to say okay this is in between like exactly in the middle of 0 and 20. So it's probably like 10 miles per hour. So that's what like 10 hours slightly less than 10. This is how a human being would figure out. So what now we have to do is we have to embed this information as part of the in context few short learning. So from the prompt in itself with the few short examples that we give it can learn. So how do we do it? So let's go to the final the most correct accurate answer and then we figure out how they have managed to do it. So first you said what is the read of the speed meter? So the yellow pointer is roughly at the middle between 80 and 100 MPH like miles per hour. The middle between 80 and 100 is 90. The speed is around 91 MPH as the pointer just passing 90 MPH miles per hour. So the yellow needle is roughly at the middle between 20 and 40 the middle\"),\n",
              " Document(page_content='at the middle between 20 and 40 the middle between 20 and 40 is 30 and the speed is around 29 and as the needle is closer to 30 miles per hour but not quite 38. Now you have given the prompt which is for the in context learning and you have given the few short examples and with that example what you are doing is in fact it is the two short example with a few short got one and you have got to and with that you are going to give this information and then say once you ask what is the read of the speed meter it is going to tell you the yellow pointer is at the long tick between 0 to 20. The long tick at the middle is long tick at the middle between 0 to 20 is 10. The speed is around 9 MPH, 9 miles per hour as a pointer is closer to 10 but not quite 10 and that is exactly the right answer. So the point here is that when you when you want to complicate or when you have like a complex image and you want to prompt them you need to give examples you need to give the enough context within'),\n",
              " Document(page_content='you need to give the enough context within within the prompt for GPT-4 vision to learn like have that in context learning and also use the few short examples that you have given and then give you the final answer. You have got one more example where you have got an uploaded image the image is a chart and you can see like the peak is here in June and if you see the peak June it is June 2022 but it always misses up this value always misses up you can see it says 3.32 and there are a lot of different steps and you can see the 3.32 coming again and again until the final one where you have the same in context few short which is the two short here. So you uploaded the graph and then said in the graph which here has the highest average gas price for the month of June. So you can see for the month of June the highest price is somewhere in 2018. You can see here in this particular graph and then you give more details about what all the elements that are available in the graph and then you give'),\n",
              " Document(page_content='that are available in the graph and then you give another example about what all the elements are in the graph and then finally it says hence the year with the highest average gas price for the month of June is 2022. So it manages to find out the right answer for you. So in this video we learned four techniques from the Microsoft paper the first technique is few the text instructions like how do you give the text instruction how do you steer the model in such a way that it gives you some answer. The second one is visual pointing or visual referral prompting where you annotate and point in the image and then it tries to figure out the third one is visual plus text prompting where you can have text as part of the image you can give the image and then upload the image also. The final one is the in context few short learning one short or two short examples we have seen some examples where two short few short example works fine. So where you upload one image one like prompt and then you'),\n",
              " Document(page_content='you upload one image one like prompt and then you upload two examples and explain what is the question and how do you arrive at the answer and then finally when you upload your particular image for the question that you uploaded it is going to figure out the answer. So this is the few four different prompt engineering techniques that we have learned from this paper for GPT-4 vision which is a multi model model. If you have got any question let me know in the comment section happy to listen to you and then share my feedback with you. See you in another video happy prompting.')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompts"
      ],
      "metadata": {
        "id": "p2W1jxOXpt-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "\n",
        "openai_api_key = \"sk-\"\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "# Map prompt\n",
        "map_template = \"\"\"The following is a set of documents\n",
        "{docs}\n",
        "Based on this list of docs, please identify the main themes\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "map_prompt = PromptTemplate.from_template(map_template)\n",
        "\n",
        "# Reduce prompt\n",
        "reduce_template = \"\"\"The following is set of summaries:\n",
        "{doc_summaries}\n",
        "Take these and distill it into a final, consolidated summary of the main themes.\n",
        "The final answer is a single paragraph of about 300 words and must be in Korean.\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_template)"
      ],
      "metadata": {
        "id": "bv9-YRZqZpQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Map-Reduce chains"
      ],
      "metadata": {
        "id": "tV52dud-pvil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 체인 축소\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# 문서 리스트를 받아서 하나의 문자열로 결합하고, 이를 LLMChain에 전달\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
        ")\n",
        "\n",
        "# 매핑된 문서를 결합하고 반복적으로 축소\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # 이것은 호출되는 최종 체인입니다.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # 문서가 `StuffDocumentsChain`의 컨텍스트를 초과하면\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # 문서를 그룹화할 최대 토큰 수.\n",
        "    token_max=4000,\n",
        ")\n",
        "\n",
        "# 2. 맵 체인\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "\n",
        "# 체인을 매핑하여 문서를 결합하고, 결과를 결합\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # 맵 체인\n",
        "    llm_chain=map_chain,\n",
        "    # 리듀스 체인\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # llm_chain에 문서를 넣을 변수 이름\n",
        "    document_variable_name=\"docs\",\n",
        "    # 출력에서 맵 단계의 결과를 반환\n",
        "    return_intermediate_steps=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "Xn-H08CYb01s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](https://i.imgur.com/za0cKc2.png)"
      ],
      "metadata": {
        "id": "3uvvDqwDeCqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run"
      ],
      "metadata": {
        "id": "2W3KU9GfpySv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_result = map_reduce_chain.run(split_docs)\n",
        "\n",
        "print(sum_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UugYRovwdXhG",
        "outputId": "c711e46a-ac84-403c-c564-419dee840708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "주어진 문서들에서 주요 주제는 GPT-4 비전 프롬프트 엔지니어링 기술, 텍스트 지침 따르기, 시각적 지시 또는 시각적 참조 프롬프팅, 퓨-샷 또는 문맥 속 학습, 그리고 시각과 텍스트의 결합입니다. 또한, 식사와 음식 관련 경험, 야외와 해안 지역 설정, 휴식과 평온, 시각적 요소 등의 주제도 확인할 수 있습니다. 이러한 주제들은 주어진 정보를 기반으로 추론되었으며, 문서에 존재하는 모든 주제를 포괄하지 않을 수 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UI using Gradio\n",
        "\n",
        "외국어 -> 한국어 가능\n",
        "\n",
        "https://www.youtube.com/watch?v=BAfOGBojiEU\n",
        "\n",
        "![](https://i.imgur.com/sm2W5jE.png)"
      ],
      "metadata": {
        "id": "YvmZxgnL9TXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "mMi0oendkNzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import re\n",
        "\n",
        "# 비디오 ID를 추출하는 함수 정의\n",
        "def extract_video_id(url):\n",
        "    # YouTube URL에서 비디오 ID를 추출하기 위한 정규 표현식\n",
        "    youtube_regex = (r'(https?://)?(www\\.)?'\n",
        "        '(youtube|youtu|youtube-nocookie)\\.(com|be)/'\n",
        "        '(watch\\?v=|embed/|v/|.+\\?v=)?([^&=%\\?]{11})')\n",
        "    youtube_pattern = re.compile(youtube_regex)\n",
        "    match = youtube_pattern.match(url)  # 정규 표현식과 URL이 매치되는지 확인\n",
        "    if not match:  # 매치되지 않으면 None 반환\n",
        "        return None\n",
        "    return match.group(6)  # 매치된 비디오 ID 반환\n",
        "\n",
        "# 요약 함수 정의\n",
        "def summarize(url):\n",
        "    yt = YouTube(url)  # YouTube 객체 생성\n",
        "\n",
        "    # 오디오 스트림만 필터링하고 첫 번째 오디오 스트림을 다운로드\n",
        "    yt.streams.filter(only_audio=True).first().download(\n",
        "        output_path='.', filename='input.mp3')\n",
        "\n",
        "    result = model.transcribe(\"input.mp3\")  # 오디오 파일을 텍스트로 변환\n",
        "\n",
        "    # 텍스트를 분할하고 각 부분을 Document 객체로 변환\n",
        "    docs = [Document(page_content=x) for x in text_splitter.split_text(result[\"text\"])]\n",
        "    split_docs = text_splitter.split_documents(docs)  # 분할된 문서들을 더 작은 부분으로 나눔\n",
        "\n",
        "    sum_result = map_reduce_chain.run(split_docs)  # 분할된 문서들을 처리하고 요약\n",
        "\n",
        "    video_id = extract_video_id(url)  # 비디오 ID 추출\n",
        "    # YouTube 비디오를 임베드하는 HTML 코드 생성\n",
        "    embed = f\"\"\"<iframe width='560' height='315' src='https://www.youtube.com/embed/{video_id}' frameborder='0' allowfullscreen></iframe>\"\"\"\n",
        "\n",
        "    return sum_result, embed  # 요약 결과와 임베드 코드 반환\n",
        "\n",
        "# Gradio 인터페이스 생성\n",
        "demo = gr.Interface(\n",
        "    fn=summarize,  # 요약 함수 설정\n",
        "    inputs=gr.Textbox(label=\"URL\"),  # URL 입력 텍스트 박스\n",
        "    outputs=[gr.TextArea(label=\"Summary\"), gr.HTML()],  # 요약과 HTML 출력 영역\n",
        ")\n",
        "\n",
        "# 대기열을 추가하고 인터페이스를 실행\n",
        "demo.queue().launch(debug=True, share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v8qQK-xW9ZrS",
        "outputId": "cebcd64b-132f-4b1a-ffd9-fbfdd8a7c24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://13befeb15c750212da.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://13befeb15c750212da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/button.py:89: UserWarning: Using the update method is deprecated. Simply return a new object instead, e.g. `return gr.Button(...)` instead of `return gr.Button.update(...)`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 59, in load_audio\n",
            "    out = run(cmd, capture_output=True, check=True).stdout\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
            "    raise CalledProcessError(retcode, process.args,\n",
            "subprocess.CalledProcessError: Command '['ffmpeg', '-nostdin', '-threads', '0', '-i', 'input.mp3', '-f', 's16le', '-ac', '1', '-acodec', 'pcm_s16le', '-ar', '16000', '-']' returned non-zero exit status 255.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 406, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1554, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1192, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 659, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-32-d51b848a2ac5>\", line 20, in summarize\n",
            "    result = model.transcribe(\"input.mp3\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 121, in transcribe\n",
            "    mel = log_mel_spectrogram(audio, padding=N_SAMPLES)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 140, in log_mel_spectrogram\n",
            "    audio = load_audio(audio)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 61, in load_audio\n",
            "    raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
            "RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'input.mp3':\n",
            "  Metadata:\n",
            "    major_brand     : dash\n",
            "    minor_version   : 0\n",
            "    compatible_brands: iso6mp41\n",
            "    creation_time   : 2023-02-28T06:50:12.000000Z\n",
            "  Duration: 03:53:47.41, start: 0.000000, bitrate: 48 kb/s\n",
            "  Stream #0:0(eng): Audio: aac (HE-AAC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 0 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2023-02-28T06:50:12.000000Z\n",
            "      handler_name    : ISO Media file produced by Google Inc.\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
            "Output #0, s16le, to 'pipe:':\n",
            "  Metadata:\n",
            "    major_brand     : dash\n",
            "    minor_version   : 0\n",
            "    compatible_brands: iso6mp41\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0(eng): Audio: pcm_s16le, 16000 Hz, mono, s16, 256 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2023-02-28T06:50:12.000000Z\n",
            "      handler_name    : ISO Media file produced by Google Inc.\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 pcm_s16le\n",
            "size=       1kB time=00:00:00.00 bitrate=N/A speed=N/A    \rsize=    4760kB time=00:02:32.27 bitrate= 256.1kbits/s speed= 305x    \rsize=    9770kB time=00:05:12.58 bitrate= 256.0kbits/s speed= 313x    \rsize=   14855kB time=00:07:55.31 bitrate= 256.0kbits/s speed= 317x    \rsize=   19968kB time=00:10:38.91 bitrate= 256.0kbits/s speed= 319x    \rsize=   21801kB time=00:11:37.57 bitrate= 256.0kbits/s speed= 320x    \n",
            "video:0kB audio:21801kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\n",
            "Exiting normally, received signal 2.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Killing tunnel 127.0.0.1:7860 <> https://13befeb15c750212da.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이렇게 만든 코드를 허깅페이스에 업로드하기\n",
        "https://www.phind.com/agent?cache=clnq2we7w0004l908wcpuuhhy\n"
      ],
      "metadata": {
        "id": "sU_b1dv4d2_X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GnRCeFOF_Qrn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}